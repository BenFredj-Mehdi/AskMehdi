{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dceb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "# Install required packages programmatically (works in Colab or local Python)\n",
    "def pip_install(pkgs):\n",
    "    print(\"Installing:\", pkgs)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "    print(\"Done installing.\")\n",
    "\n",
    "pip_install([\n",
    "    \"langchain\",\n",
    "    \"langchain-core\",\n",
    "    \"langchain-community\",\n",
    "    \"langchain-text-splitters\",\n",
    "    \"langchain-groq\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",\n",
    "    \"pypdf\",\n",
    "    \"python-dotenv\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056dd1f",
   "metadata": {},
   "source": [
    "# **Initialisation du Chatbot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc654e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, textwrap\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Hardcoded API key (replace with your own) ---\n",
    "HARD_GROQ_API_KEY = \"REDACTED_FOR_PRIVACY\"\n",
    "os.environ[\"GROQ_API_KEY\"] = HARD_GROQ_API_KEY\n",
    "\n",
    "CHAT_MODEL = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 4\n",
    "\n",
    "# Core profile text used as fallback context (edit to your info)\n",
    "personal_profile = \"\"\"\n",
    "Name: Mehdi Ben Fredj\n",
    "Role: AI Engineering Student\n",
    "Location: Tunis, Tunisie \n",
    "Email: mehdi.benfredj15@gmail.com\n",
    "Skills: Python, LangChain, LLMOps, RAG, MLOps, FastAPI, Cloud (AWS/GCP)\n",
    "Experience:\n",
    "- Built RAG chatbots for personal/portfolio sites.\n",
    "- Deployed LLM microservices behind FastAPI with autoscaling.\n",
    "- Integrated PDF/TXT ingestion for personal knowledge bases.\n",
    "- Build a Personal Portfolio\n",
    "Education:\n",
    "- TEK-UP University 2023-2028\n",
    "Projects:\n",
    "- Personal website chatbot answering from CV and portfolio.\n",
    "- PDF QA system with multi-file ingestion.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7c676",
   "metadata": {},
   "source": [
    "# **Load CV (For RAG):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def build_text_chunks(raw_docs: List[Document]):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    return splitter.split_documents(raw_docs)\n",
    "\n",
    "\n",
    "def load_profile_docs(profile_text: str) -> List[Document]:\n",
    "    return [Document(page_content=profile_text, metadata={\"source\": \"profile\"})]\n",
    "\n",
    "\n",
    "def docs_from_upload(paths: List[str]) -> List[Document]:\n",
    "    from pathlib import Path\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        ext = Path(p).suffix.lower()\n",
    "        if ext == \".txt\":\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                docs.append(Document(page_content=f.read(), metadata={\"source\": p}))\n",
    "        elif ext == \".pdf\":\n",
    "            reader = PdfReader(p)\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": p}))\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file: {p}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_vectorstore(all_docs: List[Document]) -> FAISS:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    return FAISS.from_documents(all_docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and build vector store\n",
    "# Safety: define helpers here if not already run\n",
    "try:\n",
    "    load_profile_docs\n",
    "    build_text_chunks\n",
    "    build_vectorstore\n",
    "    docs_from_upload\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    def build_text_chunks(raw_docs: List[Document]):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "        )\n",
    "        return splitter.split_documents(raw_docs)\n",
    "\n",
    "    def load_profile_docs(profile_text: str) -> List[Document]:\n",
    "        return [Document(page_content=profile_text, metadata={\"source\": \"profile\"})]\n",
    "\n",
    "    def docs_from_upload(paths: List[str]) -> List[Document]:\n",
    "        docs = []\n",
    "        for p in paths:\n",
    "            ext = Path(p).suffix.lower()\n",
    "            if ext == \".txt\":\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    docs.append(Document(page_content=f.read(), metadata={\"source\": p}))\n",
    "            elif ext == \".pdf\":\n",
    "                reader = PdfReader(p)\n",
    "                text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "                docs.append(Document(page_content=text, metadata={\"source\": p}))\n",
    "            else:\n",
    "                print(f\"Skipping unsupported file: {p}\")\n",
    "        return docs\n",
    "\n",
    "    def build_vectorstore(all_docs: List[Document]) -> FAISS:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "        return FAISS.from_documents(all_docs, embedding=embeddings)\n",
    "\n",
    "# Put your CV path here (e.g., \"/content/cv.pdf\" in Colab)\n",
    "user_files = [\"sample_cv.pdf\"]\n",
    "\n",
    "profile_docs = load_profile_docs(personal_profile)\n",
    "uploaded_docs = docs_from_upload(user_files)\n",
    "\n",
    "print(f\"Loaded profile docs: {len(profile_docs)}\")\n",
    "print(f\"Loaded uploaded docs: {len(uploaded_docs)} -> {user_files}\")\n",
    "\n",
    "all_docs = build_text_chunks(profile_docs + uploaded_docs)\n",
    "print(f\"Total chunks: {len(all_docs)}\")\n",
    "\n",
    "vs = build_vectorstore(all_docs)\n",
    "print(\"Vector store ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793da77",
   "metadata": {},
   "source": [
    "# **Build LLM + retriever and start chat:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1774930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LLM + retriever and start chat\n",
    "\n",
    "llm = ChatGroq(model=CHAT_MODEL, temperature=0.1)\n",
    "retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "system_prompt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    You are Mehdi's secretary. Be concise and answer using the provided context (profile + docs).\n",
    "    If you do not know, say you do not know. Mention the document source name when you can.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    print(\"Hello, I'm Mehdi's secretary. Ask me anything about Mehdi from his CV.\")\n",
    "    while True:\n",
    "        q = input(\"You: \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "\n",
    "        docs = retriever.invoke(q)  # modern API\n",
    "        context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "        history_text = \"\\n\".join([f\"User: {h[0]}\\nAssistant: {h[1]}\" for h in chat_history])\n",
    "\n",
    "        user_prompt = f\"\"\"Context:\\n{context}\\n\\nChat history (if any):\\n{history_text}\\n\\nQuestion: {q}\\nAnswer:\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_prompt),\n",
    "        ]\n",
    "\n",
    "        resp = llm.invoke(messages)\n",
    "        answer = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "        chat_history.append((q, answer))\n",
    "        print(f\"Bot: {answer}\\n\")\n",
    "\n",
    "chat()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
